{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Diffusion 3D image 2 image\n","\n","This example shows how you can train 3D image2image with this sub-repository"]},{"cell_type":"markdown","metadata":{},"source":["Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1wand data/T2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","\n","assert Path(\n","    \"data/T1\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\"\n","assert Path(\n","    \"data/T2\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\"\n","assert Path(\n","    \"data/T1/IXI012-HH-1211-T1.nii.gz\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\"\n","assert Path(\n","    \"data/T2/IXI012-HH-1211-T2.nii.gz\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\""]},{"cell_type":"markdown","metadata":{},"source":["Make a csv or xlsx with the data pairs.\n","\n","In one row we have matching images\n","\n","\n","Head: idx | Phase | Path | [NAME]*"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["d = {\"Phase\": [], \"Path\": [], \"T1w\": [], \"T2w\": []}\n","import random\n","import nibabel as nib\n","import pandas as pd\n","\n","\n","def rest_path(x: Path | None):\n","    if x is None:\n","        return None\n","    par = x.parent.name\n","    return par + \"/\" + x.name\n","\n","\n","random.seed(42)\n","for t2w_path in Path(\"data/T2\").iterdir():\n","    split = random.random()\n","    if split < 0.8:\n","        phase = \"train\"\n","    elif split < 0.85:\n","        phase = \"val\"\n","    else:\n","        phase = \"test\"\n","    if Path(str(t2w_path).replace(\"T2\", \"T1\")).exists():\n","        t1w_path = Path(str(t2w_path).replace(\"T2\", \"T1_resampled\"))\n","\n","        d[\"Phase\"].append(phase)\n","        d[\"Path\"].append(t2w_path.parent.parent.absolute())\n","        d[\"T1w\"].append(rest_path(t1w_path))\n","        d[\"T2w\"].append(rest_path(t2w_path))\n","df = pd.DataFrame(data=d)\n","print(df.head())\n","xls = Path(\"data\", \"train_paired.xlsx\")\n","df.to_excel(xls)"]},{"cell_type":"markdown","metadata":{},"source":["This dataset is registered but not yet resampled to the same pixel space. We fix this by using resample_from_to.\n","For brain translation we usually use skull-striping, to remove the brain. We skip this for simplicity.\n","\n","Warning this will will take a while!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nibabel.processing as nip\n","\n","\n","for t2w_path in Path(\"data/T2\").iterdir():\n","    t1w_path = Path(str(t2w_path).replace(\"T2\", \"T1\"))\n","    t1w_path_out = Path(str(t2w_path).replace(\"T2\", \"T1_resampled\"))\n","    t1w_path_out.parent.mkdir(exist_ok=True)\n","    if not t1w_path.exists():\n","        continue\n","    if t1w_path_out.exists():\n","        print(\"skip:\", t1w_path, \"    \", end=\"\\r\")\n","        continue\n","    t2w_nib = nib.load(t2w_path)\n","    t2w_arr = t2w_nib.get_fdata()\n","    t1w_nib: nib.nifti1.Nifti1Image = nib.load(t1w_path)\n","    # t1w_nib.header.get_zooms() == t2w_nib.header.get_zooms():\n","    t1w_nib = nip.resample_from_to(t1w_nib, t2w_nib, 0, cval=0, mode=\"constant\")\n","    nib.save(t1w_nib, t1w_path_out)\n","    print(\"save:\", t1w_path, \"    \", end=\"\\r\")"]},{"cell_type":"markdown","metadata":{},"source":["Make a new config file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_mode = False\n","a = f\"\"\"batch_size: 1\n","experiment_name:'Diffusion_3D_Brain_T2w_to_T1w' \n","lr:0.0002 \n","batch_size:1 \n","batch_size_val:1 \n","num_epochs:2000 \n","num_cpu:16 \n","target_patch_shape:[96, 96, 96] #24 GB\n","flip:True \n","gpus:[0] \n","new:False \n","prevent_nan:False \n","volumes:True \n","dim_multiples:'1, 2, 4, 8' \n","channels:64 \n","cpu:False \n","start_epoch:0 \n","log_dir:'logs_diffusion3D' \n","model_name:'unet' \n","L2_loss:False \n","linear:False \n","learned_variance:False #Do not use, because it it not implemented in DDIM (only in DDPM)\n","timesteps:1000 \n","image_mode:{image_mode} # False: predict noise; True: predict x_0 (initial image)\n","conditional_dimensions:1 \n","\n","image_dropout: 0.0 # Must be set 0.0 > for classifier free guidance\n","dataset: {str(xls)}\n","dataset_val: {str(xls)}\n","\n","output_rows : T1w\n","input_rows : T2w\n","conditional_label_size: 0\"\"\"\n","f = open(\"configs/diffusion3D_brain.conf\", \"a\")\n","f.write(a)\n","f.close()\n","print(a)"]},{"cell_type":"markdown","metadata":{},"source":["You can now start the training with \n","```\n","python train3D.py --config configs/diffusion3D_brain.conf\n","```\n","\n","We recommend this to start in a terminal.\n","\n","If you ssh to an linux sever for your Deep Learning, consider \"tmux\", so you can close your terminal without stopping your training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if input(\"We recommend this to start in a terminal. Start anyway! (yes/no)\") == \"yes\":\n","    !python train3D.py --config configs/diffusion3D_brain.conf\n","else:\n","    print('skip')"]},{"cell_type":"markdown","metadata":{},"source":["## Tensorboard\n","\n","\n","We use Tensorboard. Activate it with in a terminal:\n","```\n","cd /path/to/this/folder\n","tensorboard --logdir logs_diffusion3D --port 6008 --samples_per_plugin=images=100\n","```\n","cd to the git path. The **logs_diffusion3D** will be created when you run the diffusion.\n","You can see training and images got to \"http://localhost:6008\" in your web browser. The loss of diffusion converges only in the beginning of the training, the model will still improve.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!tensorboard --logdir logs_diffusion3D --port 6008 --samples_per_plugin=images=100"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["logs_diffusion3D/Diffusion_3D_Diffusion_3D_Brain_T2w_to_T1w/version_2/checkpoints/epoch=521-step=238032-train_All=0.00000000_latest.ckpt\n"]}],"source":["from train3D import Diffusion3D\n","import torch\n","from torch import Tensor\n","from utils.nii import NII\n","import glob\n","import os\n","\n","files = glob.glob(\n","    \"logs_diffusion3D/Diffusion_3D_Diffusion_3D_Brain_T2w_to_T1w/version_*/checkpoints/epoch=*-step=*-train_All=*_latest.ckpt\"\n",")\n","files.sort(key=os.path.getmtime)\n","model_path = files[-1]\n","print(model_path)\n","diffusion = Diffusion3D.load_from_checkpoint(model_path)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from utils.preprocessing import pad_size3D, run_model, revert_iso_to_original\n","\n","\n","def prepare_nii(mri_path):\n","    nii_iso = NII.load(mri_path, False, 0)\n","    # nii_iso = nii.rescale((1, 1, 1), verbose=True).reorient((\"R\", \"I\", \"P\"))\n","    nii_iso /= nii_iso.max() * 1.1\n","    nii_iso.clamp_(min=0)\n","    nii_iso = nii_iso * 2 - 1\n","    arr = Tensor(nii_iso.get_array().astype(float))\n","    arr, padding = pad_size3D(arr)\n","    return arr, padding, nii_iso"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def translate(mri_path, save_path=None, use_cpu=False, steps=25, eta=1):\n","    arr, padding, nii_iso = prepare_nii(mri_path)\n","    if torch.cuda.is_available() and not use_cpu:\n","        # If you run out of memory use max_shape= (arr.shape[-1]*arr.shape[-2]*arr.shape[-3])//2\n","        print(arr.max(), arr.min())\n","        out_arr = run_model(diffusion, conditional=arr, gpu=True, eta=eta, w=0, steps=steps)\n","    else:\n","        print(\"!!! Fall back to less steps on CPU instead of 25 GPU. !!!\")\n","        out_arr = run_model(diffusion, conditional=arr, gpu=False, eta=eta, w=0, steps=steps // 10)\n","    out_arr += 1000\n","    out_arr /= 20\n","    other_nii_iso = nii_iso.set_array(out_arr.numpy())\n","    other_nii = revert_iso_to_original(other_nii_iso, None, padding)\n","    if save_path is not None:\n","        other_nii.save(save_path)\n","    return other_nii"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'set_array' with different dtype: from int16 to float64\n","tensor(0.8182) tensor(-1.)\n","Run Diffusion; steps = 100, eta = 1.0, w = 0\n"]},{"name":"stderr","output_type":"stream","text":["sampling ddim eta=1.0:   0%|          | 0/100 [00:03<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mri_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/T2/IXI054-Guys-0707-T2.nii.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m out_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/IXI054-Guys-0707-T1_desc-translated.nii.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m translate(mri_path, save_path\u001b[39m=\u001b[39;49mout_path, steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n","\u001b[1;32m/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb Cell 19\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(mri_path, save_path, use_cpu, steps, eta)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m use_cpu:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# If you run out of memory use max_shape= (arr.shape[-1]*arr.shape[-2]*arr.shape[-3])//2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(arr\u001b[39m.\u001b[39mmax(), arr\u001b[39m.\u001b[39mmin())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     out_arr \u001b[39m=\u001b[39m run_model(diffusion, conditional\u001b[39m=\u001b[39;49marr, gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, eta\u001b[39m=\u001b[39;49meta, w\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, steps\u001b[39m=\u001b[39;49msteps)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/tutorial_image2image_3D.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m!!! Fall back to less steps on CPU instead of 25 GPU. !!!\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/utils/preprocessing.py:54\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model, conditional, gpu, eta, w, steps, depth, max_shape)\u001b[0m\n\u001b[1;32m     52\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([out1[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :s1, :], out2[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, padding:, :]], \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     53\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m---> 54\u001b[0m out \u001b[39m=\u001b[39m __run_model(model, conditional, gpu, eta, w, steps, depth\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[0;32m/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/utils/preprocessing.py:73\u001b[0m, in \u001b[0;36m__run_model\u001b[0;34m(model, conditional, gpu, eta, w, steps, depth)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, Diffusion3D):\n\u001b[1;32m     72\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdiffusion_net\n\u001b[0;32m---> 73\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward_ddim(\u001b[39m1\u001b[39;49m, \u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m1000\u001b[39;49m, \u001b[39m1000\u001b[39;49m \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m steps), eta\u001b[39m=\u001b[39;49meta, x_conditional\u001b[39m=\u001b[39;49mconditional, w\u001b[39m=\u001b[39;49mw)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     74\u001b[0m out \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     75\u001b[0m out \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/media/data/robert/code/Readable-Conditional-Denoising-Diffusion/diffusion3d/pl_models/diffusion/diffusion.py:398\u001b[0m, in \u001b[0;36mDiffusion.forward_ddim\u001b[0;34m(self, batch_size, intermediate, eta, x_conditional, label, embedding, noise, return_noise, w, progressbar)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mimage_mode:\n\u001b[1;32m    397\u001b[0m     x0_t: Tensor \u001b[39m=\u001b[39m (img \u001b[39m-\u001b[39m e_t \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m a_bar_t)\u001b[39m.\u001b[39msqrt()) \u001b[39m/\u001b[39m a_bar_t\u001b[39m.\u001b[39msqrt()\n\u001b[0;32m--> 398\u001b[0m out_intermediate_list\u001b[39m.\u001b[39mappend(x0_t\u001b[39m.\u001b[39;49mcpu())\n\u001b[1;32m    400\u001b[0m \u001b[39m# sqrt propagated\u001b[39;00m\n\u001b[1;32m    401\u001b[0m c1 \u001b[39m=\u001b[39m eta \u001b[39m*\u001b[39m ((\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m a_bar_t_next) \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m a_bar_t) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (a_bar_t \u001b[39m/\u001b[39m a_bar_t_next)))\u001b[39m.\u001b[39msqrt()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["mri_path = \"data/T2/IXI054-Guys-0707-T2.nii.gz\"\n","out_path = \"data/IXI054-Guys-0707-T1_desc-translated.nii.gz\"\n","translate(mri_path, save_path=out_path, steps=25)  # use_cpu=True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
