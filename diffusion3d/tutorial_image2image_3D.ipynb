{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Diffusion 3D image 2 image\n","\n","This example shows how you can train 3D image2image with this sub-repository"]},{"cell_type":"markdown","metadata":{},"source":["Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1wand data/T2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","\n","assert Path(\n","    \"data/T1\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\"\n","assert Path(\n","    \"data/T2\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\"\n","assert Path(\n","    \"data/T1/IXI012-HH-1211-T1.nii.gz\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\"\n","assert Path(\n","    \"data/T2/IXI012-HH-1211-T2.nii.gz\"\n",").exists(), \"Go to http://brain-development.org/ixi-dataset/ and download T1 images (all images) and T2 images (all images) and put them in the folder /data/T1 and data/T2\""]},{"cell_type":"markdown","metadata":{},"source":["Make a csv or xlsx with the data pairs.\n","\n","In one row we have matching images\n","\n","\n","Head: idx | Phase | Path | [NAME]*"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["d = {\"Phase\": [], \"Path\": [], \"T1w\": [], \"T2w\": []}\n","import random\n","import nibabel as nib\n","import pandas as pd\n","\n","\n","def rest_path(x: Path | None):\n","    if x is None:\n","        return None\n","    par = x.parent.name\n","    return par + \"/\" + x.name\n","\n","\n","random.seed(42)\n","for t2w_path in Path(\"data/T2\").iterdir():\n","    split = random.random()\n","    if split < 0.8:\n","        phase = \"train\"\n","    elif split < 0.85:\n","        phase = \"val\"\n","    else:\n","        phase = \"test\"\n","    if Path(str(t2w_path).replace(\"T2\", \"T1\")).exists():\n","        t1w_path = Path(str(t2w_path).replace(\"T2\", \"T1_resampled\"))\n","\n","        d[\"Phase\"].append(phase)\n","        d[\"Path\"].append(t2w_path.parent.parent.absolute())\n","        d[\"T1w\"].append(rest_path(t1w_path))\n","        d[\"T2w\"].append(rest_path(t2w_path))\n","df = pd.DataFrame(data=d)\n","print(df.head())\n","xls = Path(\"data\", \"train_paired.xlsx\")\n","df.to_excel(xls)"]},{"cell_type":"markdown","metadata":{},"source":["This dataset is registered but not yet resampled to the same pixel space. We fix this by using resample_from_to.\n","For brain translation we usually use skull-striping, to remove the brain. We skip this for simplicity.\n","\n","Warning this will will take a while!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nibabel.processing as nip\n","\n","\n","for t2w_path in Path(\"data/T2\").iterdir():\n","    t1w_path = Path(str(t2w_path).replace(\"T2\", \"T1\"))\n","    t1w_path_out = Path(str(t2w_path).replace(\"T2\", \"T1_resampled\"))\n","    t1w_path_out.parent.mkdir(exist_ok=True)\n","    if not t1w_path.exists():\n","        continue\n","    if t1w_path_out.exists():\n","        print(\"skip:\", t1w_path, \"    \", end=\"\\r\")\n","        continue\n","    t2w_nib = nib.load(t2w_path)\n","    t2w_arr = t2w_nib.get_fdata()\n","    t1w_nib: nib.nifti1.Nifti1Image = nib.load(t1w_path)\n","    # t1w_nib.header.get_zooms() == t2w_nib.header.get_zooms():\n","    t1w_nib = nip.resample_from_to(t1w_nib, t2w_nib, 0, cval=0, mode=\"constant\")\n","    nib.save(t1w_nib, t1w_path_out)\n","    print(\"save:\", t1w_path, \"    \", end=\"\\r\")"]},{"cell_type":"markdown","metadata":{},"source":["Make a new config file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a = f\"\"\"batch_size: 1\n","experiment_name:'Diffusion_3D_Brain_T2w_to_T1w' \n","lr:0.0002 \n","batch_size:1 \n","batch_size_val:1 \n","num_epochs:2000 \n","num_cpu:16 \n","target_patch_shape:[96, 96, 96] #24 GB\n","flip:True \n","gpus:[0] \n","new:False \n","prevent_nan:False \n","volumes:True \n","dim_multiples:'1, 2, 4, 8' \n","channels:64 \n","cpu:False \n","start_epoch:0 \n","log_dir:'logs_diffusion3D' \n","model_name:'unet' \n","L2_loss:False \n","linear:False \n","learned_variance:False #Do not use, because it it not implemented in DDIM (only in DDPM)\n","timesteps:1000 \n","image_mode:True # False: predict noise; True: predict x_0 (initial image)\n","conditional_dimensions:1 \n","\n","image_dropout: 0.0 # Must be set 0.0 > for classifier free guidance\n","dataset: {str(xls)}\n","dataset_val: {str(xls)}\n","\n","output_rows : T1w\n","input_rows : T2w\n","conditional_label_size: 0\"\"\"\n","f = open(\"configs/diffusion3D_brain.conf\", \"a\")\n","f.write(a)\n","f.close()\n","print(a)"]},{"cell_type":"markdown","metadata":{},"source":["You can now start the training with \n","```\n","python train3D.py --config configs/diffusion3D_brain.conf\n","```\n","\n","We recommend this to start in a terminal.\n","\n","If you ssh to an linux sever for your Deep Learning, consider \"tmux\", so you can close your terminal without stopping your training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if input(\"We recommend this to start in a terminal. Start anyway! (yes/no)\") == \"yes\":\n","    !python train3D.py --config configs/diffusion3D_brain.conf\n","else:\n","    print('skip')"]},{"cell_type":"markdown","metadata":{},"source":["## Tensorboard\n","\n","\n","We use Tensorboard. Activate it with in a terminal:\n","```\n","cd /path/to/this/folder\n","tensorboard --logdir logs_diffusion3D --port 6008 --samples_per_plugin=images=100\n","```\n","cd to the git path. The **logs_diffusion3D** will be created when you run the diffusion.\n","You can see training and images got to \"http://localhost:6008\" in your web browser. The loss of diffusion converges only in the beginning of the training, the model will still improve.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!tensorboard --logdir logs_diffusion3D --port 6008 --samples_per_plugin=images=100"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["logs_diffusion3D/Diffusion_3D_Diffusion_3D_Brain_T2w_to_T1w/version_0/checkpoints/epoch=147-step=67488-train_All=0.00000000_latest.ckpt\n"]}],"source":["from train3D import Diffusion3D\n","import torch\n","from torch import Tensor\n","from utils.nii import NII\n","import glob\n","import os\n","\n","files = glob.glob(\n","    \"logs_diffusion3D/Diffusion_3D_Diffusion_3D_Brain_T2w_to_T1w/version_*/checkpoints/epoch=*-step=*-train_All=*_latest.ckpt\"\n",")\n","files.sort(key=os.path.getmtime)\n","model_path = files[0]\n","print(model_path)\n","diffusion = Diffusion3D.load_from_checkpoint(model_path)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from utils.preprocessing import pad_size3D, run_model, revert_iso_to_original\n","\n","\n","def prepare_nii(mri_path):\n","    nii_iso = NII.load(mri_path, False, 0)\n","    # nii_iso = nii.rescale((1, 1, 1), verbose=True).reorient((\"R\", \"I\", \"P\"))\n","    nii_iso /= nii_iso.max() * 1.1\n","    nii_iso.clamp_(min=0)\n","    nii_iso = nii_iso * 2 - 1\n","    arr = Tensor(nii_iso.get_array().astype(float))\n","    arr, padding = pad_size3D(arr)\n","    return arr, padding, nii_iso"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def translate(mri_path, save_path=None, use_cpu=False, steps=25, eta=1):\n","    arr, padding, nii_iso = prepare_nii(mri_path)\n","    if torch.cuda.is_available() and not use_cpu:\n","        # If you run out of memory use max_shape= (arr.shape[-1]*arr.shape[-2]*arr.shape[-3])//2\n","        print(arr.max(), arr.min())\n","        out_arr = run_model(diffusion, conditional=arr, gpu=True, eta=eta, w=0, steps=steps)\n","    else:\n","        print(\"!!! Fall back to less steps on CPU instead of 25 GPU. !!!\")\n","        out_arr = run_model(diffusion, conditional=arr, gpu=False, eta=eta, w=0, steps=steps // 10)\n","    out_arr += 1000\n","    out_arr /= 20\n","    other_nii_iso = nii_iso.set_array(out_arr.numpy())\n","    other_nii = revert_iso_to_original(other_nii_iso, None, padding)\n","    if save_path is not None:\n","        other_nii.save(save_path)\n","    return other_nii"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'set_array' with different dtype: from int16 to float64\n","tensor(1.) tensor(0.)\n","Run Diffusion; steps = 25, eta = 1.0, w = 0\n"]},{"name":"stderr","output_type":"stream","text":["sampling ddim eta=1.0: 100%|██████████| 25/25 [01:27<00:00,  3.48s/it]\n"]},{"name":"stdout","output_type":"stream","text":["'set_array' with different dtype: from float64 to float32\n","Save data/IXI054-Guys-0707-T1_desc-translated.nii.gz as float32\n"]},{"data":{"text/plain":["<utils.nii.NII at 0x7fe3717333a0>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["mri_path = \"data/T2/IXI054-Guys-0707-T2.nii.gz\"\n","out_path = \"data/IXI054-Guys-0707-T1_desc-translated.nii.gz\"\n","translate(mri_path, save_path=out_path, steps=25)  # use_cpu=True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
